

# Wanシリーズ完全ガイド：誰にでも分かる解説

## シリーズ全体の物語

```
2025年3月：Wan2.2 登場
  「テキストから動画を作る」

2025年8月：Wan-S2V 登場
  「音声から動画を作る」

2025年9月：Wan-Animate 登場
  「キャラクターを好きに操作して動かす」
```

3つが組み合わさることで、**動画作成のあらゆる場面**に対応できるようになりました。

***

## 論文1：Wan2.2 を分かりやすく

### 簡単説明

**「テキストを書くと、本当の動画が出てくるAI」**

例：
```
入力：「猫がビーチで遊んでいる」
出力：その通りの動画（5秒）
```

### 何がすごいのか

#### 1. 品質が映画レベル

```
昔のAI：
「猫？ぼやぼやしてる。動きも不自然」

Wan2.2：
「猫！はっきり見える。動きもスムーズ。映画みたい」
```

#### 2. 速い

```
以前：1本の動画に1時間
今：15秒で完成
```

#### 3. 誰でも使える（安い GPU でOK）

```
以前：100万円のプロ用 GPU 必須
今：10万円の一般的な GPU（RTX 4090）で動く
```

### どうやってるのか

**秘密1：「2人の職人」**

初期段階（ノイズが多い）：
```
「大まかな構図」担当の職人が作業
「ここに猫がいるな」くらいで充分
```

後期段階（ノイズが少ない）：
```
「細かいディテール」担当の職人が引き継ぎ
「猫の毛並び、光の反射」を完璧に
```

**秘密2：「圧縮」**

```
元の動画：2GB
圧縮版：10MB以下

でも品質は落ちない

理由：「ピクセル」ではなく「意味」を圧縮
```

**秘密3：「前フレーム情報を保持」**

```
フレーム1を作る
  ↓ 「この情報を保存」
フレーム2を作る時、フレーム1の情報を参考
  ↓ 「この情報を保存」
フレーム3を作る時、フレーム2の情報を参考
  ↓
結果：フレーム間が自然につながる
```

### 論文の主張

```
「Sora（OpenAI）と同等かそれ以上の品質」
「オープンソースなので誰でも使える」
```

### 試せるか

**YES！** 以下で無料で試用可能：
- wan.video
- HuggingFace Space

***

## 論文2：Wan-S2V を分かりやすく

### 簡単説明

**「音声を入力すると、その音声に合わせた動画が出てくるAI」**

例：
```
入力：
  - 音声ファイル「こんにちは」と喋ってる音
  - 写真「この女性の顔」

出力：その女性が「こんにちは」と喋ってる動画
```

### 何がすごいのか

#### 1. 口の動きが完璧に同期

```
昔のAI：
「音とズレている」
「表情がぎこちない」

Wan-S2V：
「口の動きが音と完全一致」
「表情も自然」
```

#### 2. 何語でも対応（推定）

```
「日本語」「英語」「中国語」
すべて対応可能と推測
```

#### 3. 表情や姿勢も自動調整

```
単なる「リップシンク」ではなく、
「全身の動き」「表情」「感情」まで自動生成
```

### どんなことに使える

#### YouTuber / TikToker

```
音声を録音 → Wan-S2V → 動画に変換
→ 自動で背景動画生成

結果：撮影不要、編集も不要
```

#### 言語学習

```
「英語の発音」を教えるビデオ
AIキャラクターが「実際の口の動き」で示す
```

#### アバター

```
VTuber が「別のアバター」で配信したい場合
音声だけで、キャラクターの動きが自動生成
```

#### 映画・TV制作

```
俳優の動きを記録 → Wan-S2V → CGキャラクターに転移
```

### Wan2.2 との違い

| 項目 | Wan2.2 | Wan-S2V |
|------|--------|---------|
| 入力 | テキスト | 音声 + 画像 |
| 使い道 | 「こんな動画作って」 | 「この音声に合わせて」 |
| 得意分野 | 風景・動物・物 | 人間の表現 |
| 制御性 | テキスト指示で調整 | 音声の内容で自動調整 |

### 技術的な工夫

**音声を「理解」する**

```
単なる「音の波形」ではなく、
「何を言ってるのか」「どんな感情か」を判定

例：
  「こんにちは」（明るい声）→ 笑顔で喋る
  「こんにちは」（暗い声）→ 悲しい表情で喋る
```

**骨格信号を使う**

```
人間の動きを「骨の位置」で表現
骨の動きに基づいて、顔・手・体全体を動かす
```

**照明・色調を自動調整**

```
背景が「暗い」→ キャラクターも暗く調整
背景が「赤くなっている」→ キャラクターも赤調整

結果：「シーンに溶け込んだ」自然な合成
```

### 論文の比較対象

```
Hunyuan-Avatar：強いライバル
Omnihuman：強いライバル

Wan-S2V が「これらより良い」と主張
```

### 実際に試す

**2025年9月5日から公開開始**

```
GitHub: Wan-Video/Wan2.2
コマンド例：
  python generate.py --task s2v-14B \
    --image face.jpg --audio voice.wav
```

***

## 論文3：Wan-Animate を分かりやすく

### 簡単説明

**「キャラクターの写真と、別の人の動き動画を入れると、キャラクターが同じように動く動画が出てくるAI」**

例：
```
入力1：写真「推しのキャラクターのイラスト」
入力2：動画「ダンスを踊っている人」

出力：そのキャラクターが、同じダンスを踊ってる動画
```

### 2つのモード

#### モード1：アニメーションモード

```
「キャラクターをまるで人形のように操作」

例：
  入力動画：バレエダンサーが踊ってる
  → キャラクターが同じポーズで踊る

使い道：
  - キャラクターにダンスをさせる
  - キャラクターに演技をさせる
```

#### モード2：置換モード（Replacement）

```
「シーン内の人物を、別のキャラクターに置き換え」

例：
  シーン：男性が仕事してる映像
  → その男性をアニメキャラクターに置き換え
  → キャラクターが同じように仕事をしてる動画に

使い道：
  - 映画のキャラクター変更
  - ゲームのカットシーン作成
```

### 何がすごいのか

#### 1. 表情や細かい動きも保持

```
昔の「置き換え」：
「動きだけ」が転移
→ 表情がない、ぎこちない

Wan-Animate：
「動き」「表情」「感情」すべてが転移
→ 自然で、まるで最初からそのキャラだったかのよう
```

#### 2. 光や色合いまで自動調整

```
元のシーン：暗い部屋、青い照明
→ キャラクターもその照明に合わせて自動変色

結果：「ずっとそこにいたかのような」自然な合成
```

#### 3. 複数キャラクターへの応用（推定）

```
1人のキャラだけでなく、
複数キャラの相互作用もやがて可能？
```

### 使い道

#### ゲーム制作

```
映画的なカットシーン：
  「実写の俳優の演技」
  → 「ゲームのキャラクター」に変換
  
結果：低予算で映画級のカットシーン
```

#### アニメ制作

```
原画：「参考となるダンスの映像」
→ Wan-Animate：「キャラクターのダンス版」に変換
→ アニメーターの手作業が大幅削減
```

#### VTuber

```
VTuber が別のアバターに変身したい場合
自分の動きが、新キャラクターに自動転移
```

#### 広告・プロモーション

```
「推しキャラクターが実生活で活躍」する動画
→ AI で自動生成 → コスト激減
```

### Wan2.2 との違い

| 項目 | Wan2.2 | Wan-Animate |
|------|--------|------------|
| 入力 | テキスト | キャラ画像 + 動き動画 |
| 用途 | 「自由に作る」 | 「既存のキャラを操作」 |
| 得意 | 無から有を作る | 既存キャラを活用 |
| 制御性 | テキスト指示 | 参照動画で完全制御 |

### 技術的な工夫

**「骨格」を使った動き転移**

```
人間の動き：「腕がここ、脚がここ」という骨の位置で表現

アニメキャラの骨の位置も定義
→ 「同じ骨の位置」に動かす

結果：動きがそのまま転移
```

**「表情特徴」を保持**

```
元の人の「表情」を数値化
→ キャラクターの「同じ表情」に変換

例：
  元の人：眉をひそめてる
  → キャラクター：同じように眉をひそめる
```

**「ライティング LoRA」で環境統合**

```
LoRA = 「小さな調整ツール」

用途：照明・色調を自動調整
→ キャラクターが「その場所にいる」ように見える
```

### 論文の新しさ

```
「ホリスティック複製」（Holistic Replication）

= 「全体的にコピー」という意味

従来：「動きだけコピー」
新しい：「動き＋表情＋光＋色調」すべてコピー
```

### 実装状況

**コード公開予定**

論文に記載：

```
「We will open-source the code」
```

**推定リリース日**：2025年10月中（未確定）

***

## 3論文の使い分け

### シチュエーション別ガイド

#### 「自由に動画を作りたい」

→ **Wan2.2** を使う

```
例：「宇宙ステーションの風景」
例：「ドラゴンが空を飛ぶ」
```

#### 「音声に合わせた動画を作りたい」

→ **Wan-S2V** を使う

```
例：YouTubeの音声ナレーションに合わせたキャラ動画
例：音楽に合わせた歌声の動画
```

#### 「推しキャラを動かしたい」

→ **Wan-Animate** を使う

```
例：推しキャラをダンスさせる
例：推しキャラがアクション映画の主人公になる
```

***

## 3論文の共通点

### 技術的基盤

```
すべて「Wan2.2」の技術をベースに構築
→ つまり、「信頼できる基盤」の上に乗ってる
```

### オープンソース精神

```
Wan2.2：完全公開（2025年3月）
Wan-S2V：完全公開（2025年9月）
Wan-Animate：公開予定（2025年10月）

→ 業界全体で使えるように
```

### 著者の重複

```
Xin Gao, Li Hu, Siqi Hu など
同じコア研究チームが作成

→ 「家族」のような関連性
```

***

## 今後の展望

### 次に期待できること

#### 1. **Wan-3D**（推測）

```
「3D 動画生成」が次かも
→ 360度回転できる動画
```

#### 2. **Wan-Interactive**（推測）

```
「リアルタイム生成」かも
→ 配信中にリアルタイムで動画生成
```

#### 3. **Wan-Multimodal**（推測）

```
「全入力統合」かも
→ テキスト + 音声 + 画像 を同時入力
```

### リリース予想

```
2025年11月～12月：次の論文が出る可能性
```

***

## あなたへのメッセージ

### Wanシリーズが意味すること

$$
\text{「動画作成の民主化」が本格化している}
$$

```
2024年：「Soraってすごい」（でも使えない）
2025年：「Wanで自分も作れる」（無料で！）
```

### 産業への影響

| 業界 | 影響 |
|------|------|
| YouTube 創作者 | 撮影・編集不要に |
| ゲーム開発 | カットシーン自動化 |
| アニメ制作 | 原画枚数削減 |
| 広告制作 | コスト激減 |
| VTuber | アバター自由自在 |

### 今できること

1. **試してみる**
   ```
   wan.video で無料登録
   テキストで動画作成
   ```

2. **学ぶ**
   ```
   GitHub: Wan-Video/Wan2.2
   実装を読む
   ```

3. **応用を考える**
   ```
   「自分のプロジェクトに使えるか？」
   を考える
   ```

***

## 最後に

Wanシリーズは、**「基礎技術から応用まで、3ヶ月で成し遂げた」**例です。

これは：

```
・研究の高速化
・実装の効率化
・業界全体の進化
```

の象徴。

あなたも、この波に乗れます。[1][2][3]

引用:
[1] Wan: Open and Advanced Large-Scale Video Generative Models https://arxiv.org/abs/2503.20314
[2] Wan-S2V: Audio-Driven Cinematic Video Generation https://arxiv.org/abs/2508.18621
[3] [2509.14055] Wan-Animate: Unified Character Animation and ... - arXiv https://arxiv.org/abs/2509.14055
