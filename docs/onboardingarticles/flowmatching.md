以下では、DDPM（拡散モデル）と Flow Matching（特に Rectified Flow 版）を、**できるだけ厳密に**、かつ Wan 論文の記述ときちんと整合する形で整理します。

---

# 1. 生成モデリングの共通設定

入力次元を
次元数が d の実ベクトル空間とします。

* 「データ分布」を

$$
\pi_1
\ \text{on}\ \mathbb{R}^d
$$

* 「ベース分布」（ノイズ）を標準正規分布

$$
\pi_0
= \mathcal{N}(0, I_d)
$$

とします。

目標は、ベース分布からデータ分布へ写す**輸送写像**（もしくはその確率的拡張）を構成することです。
すなわち、ある写像（あるいは確率過程）を用いて

$$
x_0 \sim \pi_0
\ \Longrightarrow\
x_1 \sim \pi_1
$$

となる生成過程を設計・学習します。

---

# 2. DDPM（拡散モデル）の離散時間定式化

## 2.1 前向き拡散過程

Ho らの DDPM では、データサンプルを
と書き、その上に時間ステップ t を 1 から T まで進める**前向きマルコフ連鎖**を定義します。

前向き過程の条件付き分布は

$$
q(x_{1:T} \mid x_0)
===================

\prod_{t=1}^T
q(x_t \mid x_{t-1})
$$

とし、各ステップをガウス分布

$$
q(x_t \mid x_{t-1})
===================

\mathcal{N}
\bigl(
x_t;
\sqrt{1 - \beta_t}, x_{t-1},
\beta_t I
\bigr),
\quad
\beta_t \in (0,1)
$$

と定義します。
ここで列は「ノイズスケジュール」です。

このマルコフ連鎖を繰り返すと、大きな t では分布がほぼ標準正規分布に近づきます。

このチェーンは閉形式で

$$
q(x_t \mid x_0)
===============

\mathcal{N}
\bigl(
x_t;,
\sqrt{\bar{\alpha}_t}, x_0,,
(1 - \bar{\alpha}_t) I
\bigr)
$$

と書けます。ここで

$$
\alpha_t = 1 - \beta_t,
\qquad
\bar{\alpha}*t = \prod*{s=1}^t \alpha_s
$$

です。

## 2.2 逆向き生成過程

生成時には逆向き過程

$$
p_\theta(x_{0:T})
=================

p(x_T)
\prod_{t=1}^T
p_\theta(x_{t-1} \mid x_t)
$$

を用います。終端分布は

$$
p(x_T)
======

\mathcal{N}(0, I)
$$

とし、逆遷移を

$$
p_\theta(x_{t-1} \mid x_t)
==========================

\mathcal{N}
\bigl(
x_{t-1};,
\mu_\theta(x_t, t),,
\sigma_t^2 I
\bigr)
$$

というガウス分布でパラメタライズします。
分散に関しては、前向き過程から導かれる解析的形を用いる設計が一般的です。

## 2.3 学習目的（簡略損失）

DDPM 本来の目的関数は変分下界（ELBO）ですが、Ho らはこれが**単純なノイズ推定 MSE**に還元できることを示します。

ノイズスケジュールを使って

$$
x_t
===

\sqrt{\bar{\alpha}_t}, x_0
+
\sqrt{1 - \bar{\alpha}_t}, \varepsilon,
\quad
\varepsilon \sim \mathcal{N}(0, I)
$$

と書いたとき、ニューラルネットワークはノイズ
を予測するものとします。

簡略損失は

$$
L_{\mathrm{DDPM}}(\theta)
=========================

\mathbb{E}*{x_0 \sim \pi_1,,
\varepsilon \sim \mathcal{N}(0,I),,
t \sim \mathrm{Unif}{1,\dots,T}}
\bigl[
|
\varepsilon
-
\varepsilon*\theta(x_t, t)
|^2
\bigr]
$$

という形になります。
ここで は「ノイズ予測ネット」です。

---

# 3. 連続時間拡散モデルと確率フロー ODE

DDPM を時間ステップ幅を 0 に近づける極限で見ると、スコアベース拡散モデルと同様に**確率微分方程式（SDE）**として書くことができます。

## 3.1 前向き SDE

連続時間 t を 0 から 1 とし、前向き SDE を

$$
\mathrm{d}x_t
=============

f(x_t, t),\mathrm{d}t
+
g(t),\mathrm{d}w_t
$$

と書きます。ここで

* はドリフト
* はスケール関数
* は標準ブラウン運動

です。
これは離散時間拡散過程の極限として導出できます。

## 3.2 確率フロー ODE

Song らは、この SDE に対応する**確率フロー常微分方程式（ODE）**を

$$
\mathrm{d}x_t
=============

\Bigl(
f(x_t, t)
-
\frac{1}{2}
g(t)^2
\nabla_x
\log p_t(x_t)
\Bigr),
\mathrm{d}t
$$

と定義します。
ここで は時刻 t における分布 の確率密度です。

この ODE は、対応する SDE と**同じ周辺分布列 を持つ**ことが知られています。
したがって、この ODE を解くことでもベース分布からデータ分布までの輸送が実現できます。

---

# 4. Flow Matching の一般的定式化

Flow Matching（FM）は、Continuous Normalizing Flows（CNF）を**シミュレーションなしで学習する枠組み**として提案されています。

## 4.1 CNF（連続正規化フロー）

CNF では、連続時間 ODE

$$
\frac{\mathrm{d}x_t}{\mathrm{d}t}
=================================

v_\theta(x_t, t),
\quad
t \in [0,1],
$$

$$
x_0 \sim \pi_0
$$

を用いて、ベース分布からデータ分布への輸送を実現します。
ベクトル場 はニューラルネットワークでパラメタライズされます。

## 4.2 条件付き確率パス

Flow Matching の鍵は、「中間時刻の分布」を**任意の「確率パス族」**として指定することです。

* ベース分布 から
* データ分布 へ

の橋渡しとして、任意の条件付き分布

$$
p_t(x \mid x_0, x_1),
\quad
t \in [0,1]
$$

を定義します。
ここで は「始点サンプル」、 は「終点サンプル」です。

このとき、時刻 t の周辺分布は

$$
p_t(x)
======

\iint
p_t(x \mid x_0, x_1),
\pi_0(x_0),
\pi_1(x_1),
\mathrm{d}x_0,\mathrm{d}x_1
$$

となります。

## 4.3 目標ベクトル場の定義

条件付きパスの**平均軌道**を

$$
\tilde{x}_t(x_0, x_1)
=====================

\mathbb{E}
\bigl[
x_t
\mid
x_0, x_1
\bigr]
$$

その時間微分を**条件付き速度**

$$
v_t(x_0, x_1, t)
================

\frac{\mathrm{d}}{\mathrm{d}t}
\tilde{x}_t(x_0, x_1)
$$

とします（解析的に計算できるよう設計します）。

このとき、Lipman らは

$$
u_t(x)
======

\mathbb{E}
\bigl[
v_t(x_0, x_1, t)
\mid
x_t = x
\bigr]
$$

と定義すると、ベクトル場 が**連続の方程式**

$$
\frac{\partial}{\partial t}
p_t(x)
+
\nabla_x
\cdot
\bigl(
p_t(x), u_t(x)
\bigr)
= 0
$$

を満たし、
CNF

$$
\frac{\mathrm{d}x_t}{\mathrm{d}t}
=================================

u_t(x_t)
$$

の解 の分布が、パスで定義した と一致することを示します。

## 4.4 Flow Matching 損失

Flow Matching は、この「目標ベクトル場 」を**直接回帰**する学習則です。

サンプリング手順：

1. データサンプル
   を
   からサンプル
2. ベースサンプル
   を
   からサンプル
3. 時刻
   を区間 から一様サンプル
4. 条件付きパスから
   をサンプル（多くの場合、平均軌道＝決定論的）
5. 条件付き速度
   を計算

このとき損失関数は

$$
L_{\mathrm{FM}}(\theta)
=======================

\mathbb{E}
\bigl[
|
v_\theta(x_t, t)
-
v_t(x_0, x_1, t)
|^2
\bigr]
$$

と定義されます。

理論的には、万能近似性などの前提のもとで、最適解 は先ほどの を再現し、結果として CNF が指定した確率パスの周辺分布列 を実現します。

---

# 5. Rectified Flow：直線パスの場合

Liu らの Rectified Flow は、Flow Matching の**特別な場合**であり、
「サンプル空間上での直線パス」を用いる枠組みです。

## 5.1 直線パスと一定速度

ベースサンプル とデータサンプル を取ったとき、

**直線補間パス**を

$$
x_t
===

(1 - t), x_0
+
t, x_1,
\quad
t \in [0,1]
$$

と定めます。

このとき平均軌道はそのままパスそのものであり、時間微分は

$$
v_t(x_0, x_1)
=============

# \frac{\mathrm{d}x_t}{\mathrm{d}t}

x_1 - x_0
$$

となり、時刻 t に依存しない**一定速度**になります。

## 5.2 Rectified Flow 損失

Flow Matching の一般形に直線パスを代入すると、損失は

$$
L_{\mathrm{RF}}(\theta)
=======================

\mathbb{E}*{x_0 \sim \pi_0,,
x_1 \sim \pi_1,,
t \sim \mathrm{Unif}(0,1)}
\bigl[
|
v*\theta(
(1 - t),x_0 + t,x_1,,
t
)
-
(x_1 - x_0)
|^2
\bigr]
$$

という形に簡約されます。

Liu らは、この損失を最小化することで、多くの状況で**直線に近い輸送軌道**をもつ ODE が学習され、サンプリング効率（ステップ数）の観点で有利になることを示しています。

---

# 6. Flow Matching と DDPM の「厳密な関係」

Flow Matching は「任意の確率パス」を扱える枠組みであり、その**特別な選択として「拡散パス」を取ると、連続時間拡散モデルと一致する**ことが示されています。

## 6.1 拡散パスの定義（FM 側）

Flow Matching 論文では、たとえば次のような**ガウス型パス**を考えます。

$$
x_t
===

\alpha_t, x_1
+
\sigma_t, \varepsilon,
\quad
\varepsilon \sim \mathcal{N}(0, I)
$$

ここで
列は時間スケジュール、
列はノイズスケールで、拡散モデルの前向き過程で用いられるものと同様の選択をします。

このとき、条件付き分布は

$$
p_t(x \mid x_1)
===============

\mathcal{N}
\bigl(
x;,
\alpha_t x_1,,
\sigma_t^2 I
\bigr)
$$

となり、DDPM の前向き過程の 時刻 t における分布と同型です。

## 6.2 Drift とスコアの関係

このガウス型パスに対して、Flow Matching の目標ベクトル場

$$
u_t(x)
$$

を計算すると、
拡散モデルにおける**確率フロー ODE のドリフト**

$$
f_{\mathrm{PF}}(x, t)
=====================

## f(x, t)

\frac{1}{2}
g(t)^2
\nabla_x
\log p_t(x)
$$

と一致することが示されます（定理および補題として Lipman らが証明）。

したがって、

* 拡散パスを選んだ Flow Matching で最適ベクトル場 を学習すると
* それは「対応する拡散 SDE の確率フロー ODE のドリフト」と一致し
* 結果として、**連続時間拡散モデルと同じ周辺分布列**を持つ CNF が得られる

という「厳密な意味での同値性」が存在します。

### 6.3 ただし離散 DDPM とは目的関数が異なる

重要なのは、

* DDPM の学習目的は ELBO（ノイズ推定の MSE 版）
* Flow Matching の学習目的は「目標速度ベクトル」との MSE

であり、**目的関数の形は異なる**、という点です。

連続時間の極限かつネットワークが最適に近いとき、

* 拡散モデルのスコア推定
* Flow Matching のベクトル場回帰

が同じ確率フロー ODE を実現する、という意味で「同じ山に登っている」ことが証明されますが、
有限ステップ・有限容量のネットワークにおいては挙動が必ずしも一致するとは限りません。

---

# 7. Wan における Flow Matching の定式化（Rectified Flow 型）

Wan 論文は、トレーニング目標として**Rectified Flow 型の Flow Matching**を明示的に採用していることを記述しています。

論文中の記述を整理すると、Wan の DiT 部分は

* 潜在空間上の始点ベクトル をベース分布（通常ガウス）からサンプル

* 終点ベクトル を VAE 潜在上のデータ分布からサンプル

* 直線パス

  $$
  x_t
  ===

  t x_1
  +
  (1 - t)x_0
  $$

* 速度ベクトル

  $$
  v_t
  ===

  x_1 - x_0
  $$

を用い、テキスト条件 を加えたベクトル場

$$
u_\theta(x_t, c_t(x_t), t)
$$

を学習します。

損失関数は論文要約に明示されており、次のような形です。

$$
L_{\mathrm{Wan}}(\theta)
========================

\mathbb{E}
\bigl[
|
u_\theta(x_t, c_t(x_t), t)
-
(x_1 - x_0)
|^2
\bigr]
$$

ここで

* は Wan の Video DiT が出力する「速度予測」
* はテキスト条件（時間依存のエンコード）
* と はそれぞれ VAE 潜在空間上のベース・データサンプル

です。

これは、前節で述べた Rectified Flow 損失

$$
L_{\mathrm{RF}}(\theta)
=======================

\mathbb{E}
\bigl[
|
v_\theta(
(1 - t)x_0 + t x_1,
t
)
-
(x_1 - x_0)
|^2
\bigr]
$$

の**条件付き版**になっていることが分かります。

したがって、

> Wan における Flow Matching は、Rectified Flow（直線パス＋一定速度）をベースとし、
> テキストや他モダリティ条件を加えた条件付き CNF を学習するもの

と厳密に位置づけられます。

---

# 8. DDPM vs Flow Matching：厳密な比較まとめ

最後に、**数学的にきちんとしたレベルで**比較ポイントを整理します。

## 8.1 モデルクラス

* DDPM

  * 有限ステップ T のガウス型マルコフ連鎖
  * 前向き過程
  * 逆向き過程

* Flow Matching（含 Rectified Flow）

  * 連続時間 ODE

    $$
    \frac{\mathrm{d}x_t}{\mathrm{d}t}
    =================================

    v_\theta(x_t, t)
    $$

  * ベース分布 から データ分布 への CNF

## 8.2 学習のターゲット

* DDPM

  * 前向き拡散過程 の解析式を利用し
  * ノイズ
    またはスコア（対数密度の勾配）を推定
  * 損失は ELBO から導かれるノイズ MSE

* Flow Matching

  * 指定した確率パスの**速度ベクトル** を教師として
  * ベクトル場
    を直接回帰
  * 損失はベクトル場の二乗誤差

## 8.3 連続時間における関係

* 拡散モデルの SDE に対応する確率フロー ODE のドリフト は、
  スコア を用いて表される。

* Flow Matching が「拡散パス」を選ぶ場合、
  最適ベクトル場 は、この確率フロー ODE のドリフト と一致することが Lipman らにより示されている。

→ この意味で、**連続時間かつ最適解の極限では「同じクラスの生成過程」を学習している**といえる。

## 8.4 Rectified Flow / Wan 型との違い

* Rectified Flow は「直線パス」を選び、
  DDPM 型の「拡散パス」とは異なる確率パスを採用している。

* Wan は Rectified Flow 型の Flow Matching を採用しており、
  **拡散パスではなく直線パスに対するベクトル場**を学んでいる。

したがって、

* Wan の Flow Matching 部分は、DDPM の確率フロー ODE とは**一般には異なる輸送過程**を学んでいる。
* ただし、どちらも「ベース分布→データ分布」という生成タスクを連続時間で解いている点では同じ枠組みに属する。

---

# 9. 結論（厳密さの観点から）

1. **DDPM と Flow Matching は、同じ生成タスク（ベース分布からデータ分布への輸送）を別の定式化で解いている。**

   * DDPM：ノイズ付き SDE＋逆マルコフ連鎖
   * Flow Matching：CNF のベクトル場回帰

2. **Flow Matching が「拡散パス」を選ぶ場合、連続時間の極限で DDPM の確率フロー ODE と同値な生成過程を学習しうる**ことが、Lipman らにより理論的に示されている。

3. **Rectified Flow（直線パス）や Wan の Flow Matching は、拡散パスとは別の確率パスを選んでいるため、DDPM と「完全に同じ生成流れ」ではない**。
   それでも、CNF ベースの連続時間生成モデルとして厳密に定義されており、動画の潜在空間に対して効率的な輸送を実現している。

4. Wan について言えば、論文には

   * 直線パス
     （ ）
   * 速度ベクトル
     （）
   * MSE 損失

   を用いる Flow Matching 目標が**明示的に記載**されており、Rectified Flow 型の Flow Matching を採用していることが厳密に確認できる。
